import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import GridSearchCV, cross_val_score
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, mean_squared_error, r2_score
from sklearn.inspection import permutation_importance
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.cluster import DBSCAN
from sklearn.svm import OneClassSVM
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.holtwinters import ExponentialSmoothing
from keras.models import Sequential
from keras.layers import Dense, LSTM
from loguru import logger

class DataAnalysisModule:
    def __init__(self, config):
        self.config = config

    def train_model(self, data, target):
        logger.info("Training model...")
        try:
            if self.config['model_type'] == 'random_forest':
                model = RandomForestClassifier()
            elif self.config['model_type'] == 'gradient_boosting':
                model = GradientBoostingClassifier()
            elif self.config['model_type'] == 'neural_network':
                model = MLPClassifier()
            else:
                raise ValueError(f"Invalid model type: {self.config['model_type']}")
            model.fit(data, target)
            return model
        except Exception as e:
            logger.error(f"Error training model: {e}")
            raise

    def evaluate_model(self, model, data, target):
        logger.info("Evaluating model...")
        try:
            predictions = model.predict(data)
            accuracy = accuracy_score(target, predictions)
            precision = precision_score(target, predictions)
            recall = recall_score(target, predictions)
            f1 = f1_score(target, predictions)
            return accuracy, precision, recall, f1
        except Exception as e:
            logger.error(f"Error evaluating model: {e}")
            raise

    def analyze_feature_importance(self, model, data):
        logger.info("Analyzing feature importance...")
        try:
            importance = permutation_importance(model, data)
            return importance
        except Exception as e:
            logger.error(f"Error analyzing feature importance: {e}")
            raise

    def detect_anomalies(self, data):
        logger.info("Detecting anomalies...")
        try:
            if self.config['anomaly_detection_method'] == 'isolation_forest':
                model = IsolationForest()
            elif self.config['anomaly_detection_method'] == 'one_class_svm':
                model = OneClassSVM()
            else:
                raise ValueError(f"Invalid anomaly detection method: {self.config['anomaly_detection_method']}")
            model.fit(data)
            anomalies = model.predict(data)
            return anomalies
        except Exception as e:
            logger.error(f"Error detecting anomalies: {e}")
            raise

    def analyze_time_series(self, data):
        logger.info("Analyzing time series...")
        try:
            if self.config['time_series_analysis_method'] == 'arima':
                model = ARIMA(data, order=self.config['arima_order'])
            elif self.config['time_series_analysis_method'] == 'exponential_smoothing':
                model = ExponentialSmoothing(data)
            else:
                raise ValueError(f"Invalid time series analysis method: {self.config['time_series_analysis_method']}")
            model.fit(data)
            forecast = model.predict(len(data), len(data) + self.config['forecast_steps'])
            return forecast
        except Exception as e:
            logger.error(f"Error analyzing time series: {e}")
            raise

    def analyze_data(self, data, target):
        model = self.train_model(data, target)
        accuracy, precision, recall, f1 = self.evaluate_model(model, data, target)
        importance = self.analyze_feature_importance(model, data)
        anomalies = self.detect_anomalies(data)
        forecast = self.analyze_time_series(data)
        return model, accuracy, precision, recall, f1, importance, anomalies, forecast
