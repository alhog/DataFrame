from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.decomposition import PCA
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.model_selection import train_test_split
import numpy as np
from loguru import logger

class DataRefinementModule:
    def __init__(self, config):
        self.config = config

    def normalize_data(self, data, method='minmax'):
        logger.info(f"Normalizing data using {method} method")
        numeric_cols = data.select_dtypes(include=['float64', 'int64']).columns

        if method == 'minmax':
            scaler = MinMaxScaler()
        elif method == 'zscore':
            scaler = StandardScaler()
        else:
            raise ValueError(f"Invalid normalization method: {method}")

        for col in numeric_cols:
            data[col] = scaler.fit_transform(data[col].values.reshape(-1, 1))

        return data

    def dimensionality_reduction(self, data, method='pca', n_components=None):
        logger.info(f"Performing dimensionality reduction using {method} method")
        numeric_cols = data.select_dtypes(include=['float64', 'int64']).columns

        if method == 'pca':
            reducer = PCA(n_components=n_components)
        else:
            raise ValueError(f"Invalid dimensionality reduction method: {method}")

        reduced_data = reducer.fit_transform(data[numeric_cols])
        reduced_df = pd.DataFrame(reduced_data, columns=[f'component_{i}' for i in range(reduced_data.shape[1])])

        data = pd.concat([data, reduced_df], axis=1)
        data = data.drop(numeric_cols, axis=1)

        return data

    def feature_selection(self, data, target, method='kbest', k=10):
        logger.info(f"Performing feature selection using {method} method")
        numeric_cols = data.select_dtypes(include=['float64', 'int64']).columns

        if method == 'kbest':
            selector = SelectKBest(f_classif, k=k)
        else:
            raise ValueError(f"Invalid feature selection method: {method}")

        selected_features = selector.fit_transform(data[numeric_cols], target)
        selected_df = pd.DataFrame(selected_features, columns=numeric_cols[selector.get_support()])

        data = pd.concat([data, selected_df], axis=1)
        data = data.drop(numeric_cols, axis=1)

        return data

    def data_partitioning(self, data, target, test_size=0.2, stratify=True):
        logger.info(f"Partitioning data into train and test sets (test_size={test_size})")

        if stratify:
            train_data, test_data, train_target, test_target = train_test_split(data, target, test_size=test_size, stratify=target)
        else:
            train_data, test_data, train_target, test_target = train_test_split(data, target, test_size=test_size)

        return train_data, test_data, train_target, test_target

    def refine_data(self, data, target):
        # Data refinement pipeline
        data = self.normalize_data(data, method=self.config['normalization_method'])
        data = self.dimensionality_reduction(data, method=self.config['dimensionality_reduction_method'], n_components=self.config['n_components'])
        data = self.feature_selection(data, target, method=self.config['feature_selection_method'], k=self.config['n_features'])
        train_data, test_data, train_target, test_target = self.data_partitioning(data, target, test_size=self.config['test_size'], stratify=self.config['stratify'])

        return train_data, test_data, train_target, test_target

# Example usage
config = {
    'normalization_method': 'minmax',
    'dimensionality_reduction_method': 'pca',
    'n_components': 10,
    'feature_selection_method': 'kbest',
    'n_features': 20,
    'test_size': 0.2,
    'stratify': True
}

refinement_module = DataRefinementModule(config)
train_data, test_data, train_target, test_target = refinement_module.refine_data(enriched_data, target_data)
